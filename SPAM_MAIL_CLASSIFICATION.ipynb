{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING ALL THE REQUIRED LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_overall_error_rate=[0,0,0,0,0]\n",
    "average_false_positive_rate=[0,0,0,0,0]\n",
    "average_false_negative_rate=[0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE CREATE AN 'average_overall_error_rate' LIST SO AS TO KEEP TRACK OF THE PERFORMANCE OF EACH CLASSIFICATION ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('spambase_data.csv')\n",
    "X = dataset.iloc[:,:-1].values\n",
    "Y = dataset.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING THE DATASET\n",
    "AND STORING ALL THE INDEPENDENT VARIABLES IN 'X' (This contains the 57 features of all 4601 mails,we will use to predict whether the mail is spam or not)\n",
    "AND STORING THE DEPENDENT VARIABLE IN 'Y' (This contains the answer in terms of 1 or 0, denoting whether the mail was spam or not, for all the 4601 mails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(missing_values=np.nan, strategy = 'mean')\n",
    "imputer.fit(X)\n",
    "X = imputer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE 'SimpleImputer' FUNCTION REPLACES ALL THE MISSING VALUES IN THE DATASET WITH THE MEAN OF ALL THE VALUES IN THAT COLUMN. (This gives us a rough estimate of what the value could be and also prevents the classification model from not deriving important relationships between features and the output due to missing values)\n",
    "THE NEXT TO LINES ARE REQUIRED TO APPLY THE IMPUTER FUNCTION ON OUR INDEPENDENT VARIABLES AND THEN STORE THE UPDATED VALUES BACK IN 'X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classification_Algorithms = ['Logistic_Regression', 'K_Nearest_Neighbors', 'Support_Vector_Machine', 'Naive_Bayes', 'Random_Forest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I STORE ALL THE ALGORITHMS I CHOOSE TO APPLY ON THE DATASET IN A LIST\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "i=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I CALL THE KFOLD FUNCTION SO THAT LATER I CAN USE IT TO SPLIT OUR DATASET INTO 5 FOLDS\n",
    "THE 'I' VARIABLE IS INITIALIZED TO KEEP TRACK OF WHICH CLASSIFICATION ALGORITHM IS BEING USED IN A UPCOMING FOR LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Algorithm: Logistic_Regression | Fold no: 1 | false_positive_rate: 0.040780141843971635 | false_negative_rate: 0.12324929971988796 | overall_error_rate: 0.0727470141150923\n",
      "\n",
      "\n",
      "Algorithm: Logistic_Regression | Fold no: 2 | false_positive_rate: 0.0299625468164794 | false_negative_rate: 0.15025906735751296 | overall_error_rate: 0.08043478260869565\n",
      "\n",
      "\n",
      "Algorithm: Logistic_Regression | Fold no: 3 | false_positive_rate: 0.06398537477148081 | false_negative_rate: 0.12332439678284182 | overall_error_rate: 0.08804347826086957\n",
      "\n",
      "\n",
      "Algorithm: Logistic_Regression | Fold no: 4 | false_positive_rate: 0.05226480836236934 | false_negative_rate: 0.11271676300578035 | overall_error_rate: 0.075\n",
      "\n",
      "\n",
      "Algorithm: Logistic_Regression | Fold no: 5 | false_positive_rate: 0.050966608084358524 | false_negative_rate: 0.06267806267806268 | overall_error_rate: 0.05543478260869565\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Algorithm: K_Nearest_Neighbors | Fold no: 1 | false_positive_rate: 0.05673758865248227 | false_negative_rate: 0.1400560224089636 | overall_error_rate: 0.08903365906623235\n",
      "\n",
      "\n",
      "Algorithm: K_Nearest_Neighbors | Fold no: 2 | false_positive_rate: 0.04681647940074907 | false_negative_rate: 0.14507772020725387 | overall_error_rate: 0.08804347826086957\n",
      "\n",
      "\n",
      "Algorithm: K_Nearest_Neighbors | Fold no: 3 | false_positive_rate: 0.08043875685557587 | false_negative_rate: 0.13672922252010725 | overall_error_rate: 0.10326086956521739\n",
      "\n",
      "\n",
      "Algorithm: K_Nearest_Neighbors | Fold no: 4 | false_positive_rate: 0.0627177700348432 | false_negative_rate: 0.13005780346820808 | overall_error_rate: 0.08804347826086957\n",
      "\n",
      "\n",
      "Algorithm: K_Nearest_Neighbors | Fold no: 5 | false_positive_rate: 0.0913884007029877 | false_negative_rate: 0.1168091168091168 | overall_error_rate: 0.10108695652173913\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Algorithm: Support_Vector_Machine | Fold no: 1 | false_positive_rate: 0.030141843971631204 | false_negative_rate: 0.12324929971988796 | overall_error_rate: 0.06623235613463627\n",
      "\n",
      "\n",
      "Algorithm: Support_Vector_Machine | Fold no: 2 | false_positive_rate: 0.035580524344569285 | false_negative_rate: 0.11658031088082901 | overall_error_rate: 0.06956521739130435\n",
      "\n",
      "\n",
      "Algorithm: Support_Vector_Machine | Fold no: 3 | false_positive_rate: 0.054844606946983544 | false_negative_rate: 0.12064343163538874 | overall_error_rate: 0.08152173913043478\n",
      "\n",
      "\n",
      "Algorithm: Support_Vector_Machine | Fold no: 4 | false_positive_rate: 0.03484320557491289 | false_negative_rate: 0.09248554913294797 | overall_error_rate: 0.05652173913043478\n",
      "\n",
      "\n",
      "Algorithm: Support_Vector_Machine | Fold no: 5 | false_positive_rate: 0.04569420035149385 | false_negative_rate: 0.07977207977207977 | overall_error_rate: 0.058695652173913045\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Algorithm: Naive_Bayes | Fold no: 1 | false_positive_rate: 0.2907801418439716 | false_negative_rate: 0.06442577030812324 | overall_error_rate: 0.20304017372421282\n",
      "\n",
      "\n",
      "Algorithm: Naive_Bayes | Fold no: 2 | false_positive_rate: 0.24906367041198502 | false_negative_rate: 0.04922279792746114 | overall_error_rate: 0.16521739130434782\n",
      "\n",
      "\n",
      "Algorithm: Naive_Bayes | Fold no: 3 | false_positive_rate: 0.3180987202925046 | false_negative_rate: 0.045576407506702415 | overall_error_rate: 0.2076086956521739\n",
      "\n",
      "\n",
      "Algorithm: Naive_Bayes | Fold no: 4 | false_positive_rate: 0.23519163763066203 | false_negative_rate: 0.03757225433526012 | overall_error_rate: 0.1608695652173913\n",
      "\n",
      "\n",
      "Algorithm: Naive_Bayes | Fold no: 5 | false_positive_rate: 0.27240773286467485 | false_negative_rate: 0.02564102564102564 | overall_error_rate: 0.1782608695652174\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Algorithm: Random_Forest | Fold no: 1 | false_positive_rate: 0.024822695035460994 | false_negative_rate: 0.06722689075630252 | overall_error_rate: 0.04125950054288816\n",
      "\n",
      "\n",
      "Algorithm: Random_Forest | Fold no: 2 | false_positive_rate: 0.024344569288389514 | false_negative_rate: 0.09067357512953368 | overall_error_rate: 0.05217391304347826\n",
      "\n",
      "\n",
      "Algorithm: Random_Forest | Fold no: 3 | false_positive_rate: 0.043875685557586835 | false_negative_rate: 0.06970509383378017 | overall_error_rate: 0.05434782608695652\n",
      "\n",
      "\n",
      "Algorithm: Random_Forest | Fold no: 4 | false_positive_rate: 0.01916376306620209 | false_negative_rate: 0.06069364161849711 | overall_error_rate: 0.034782608695652174\n",
      "\n",
      "\n",
      "Algorithm: Random_Forest | Fold no: 5 | false_positive_rate: 0.02460456942003515 | false_negative_rate: 0.06267806267806268 | overall_error_rate: 0.0391304347826087\n"
     ]
    }
   ],
   "source": [
    "for ca in Classification_Algorithms:\n",
    "        #KEEPING COUNTER TO KNOW WHICH ITERATION OF THE KFOLD SPLIT WE ARE ON\n",
    "        counter=0\n",
    "        print('\\n')\n",
    "        #print(ca)#CREATING THE CLASSIFICATION ALGORITHM\n",
    "        if (ca=='Logistic_Regression'):\n",
    "            classifier = LogisticRegression(random_state=1)\n",
    "        elif (ca=='K_Nearest_Neighbors'):\n",
    "            classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p=2)\n",
    "        elif (ca=='Support_Vector_Machine'):\n",
    "            classifier = SVC(kernel = 'rbf', random_state=1)\n",
    "        elif (ca=='Naive_Bayes'):\n",
    "            classifier = GaussianNB()\n",
    "        elif (ca=='Random_Forest'):\n",
    "            classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state=1)\n",
    "        \n",
    "        for train_index, test_index in kfold.split(X):\n",
    "            print('\\n')\n",
    "            \n",
    "            counter+=1\n",
    "            #print('Fold no: '+str(counter))\n",
    "            #Splitting the dataset\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            #FEATURE SCALING\n",
    "            sc = StandardScaler()\n",
    "            X_train = sc.fit_transform(X_train)\n",
    "            X_test = sc.transform(X_test)\n",
    "        \n",
    "            #TRAINING THE CLASSIFIER        \n",
    "            classifier.fit(X_train, Y_train)\n",
    "        \n",
    "        \n",
    "            #PREDICTING THE RESULTS OF TEST SET\n",
    "            y_pred = classifier.predict(X_test)\n",
    "            cm = confusion_matrix(Y_test, y_pred)\n",
    "            tn, fp, fn, tp = confusion_matrix(Y_test, y_pred).ravel()\n",
    "            print('Algorithm: '+ca+' | Fold no: '+str(counter)+' | false_positive_rate: '+str(fp/(fp+tn))+' | false_negative_rate: '+str(fn/(fn+tp))+' | overall_error_rate: '+str((fp+fn)/(tn+fp+fn+tp)))\n",
    "            average_overall_error_rate[i] += (fp+fn)/(tn+fp+fn+tp)\n",
    "            average_false_positive_rate[i] += fp/(fp+tn)\n",
    "            average_false_negative_rate[i] += fn/(fn+tp)\n",
    "            \n",
    "        average_overall_error_rate[i] /=5\n",
    "        average_false_positive_rate[i] /=5\n",
    "        average_false_negative_rate[i] /=5\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I RUN A LOOP FOR EACH ALGORITHM WE WISH TO APPLY ON OUR DATASET.\n",
    "THE COUNTER VARIABLE IS USED TO KEEP TRACK OF WHICH FOLD IS CURRENTLY THE TEST SET. FOR EXAMPLE: IF FOLD = 1, THEN FOLD 1 IS THE TEST SET WHILE THE REST OF THE FOLD ARE IN THE TRAINING SET.\n",
    "I USE IF STATEMENT TO SEE WHICH CLASSIFICATION ALGORITHM IS TO BE PERFORMED CURRENTLY ON THE DATASET AND ACCORDINGLY I CALL THE ASSOCIATED FUNCTION.\n",
    "THEN TO DIVIDE OUR 'X' AND 'Y' VARIABLE INTO TRAINING AND TESTING SET FOR EACH FOLD I UTILISE THE KFOLD SPLIT FUNCTION .\n",
    "THE COUNTER IS INCREMENTED IN EACH LOOP OF KFOLD SPLIT TO KEEP TRACK OF WHICH ROUND OF FOLD IS CURRENTLY GOING ON.\n",
    "THEN WE USE THE OUTPUT OF THE KFOLD SPLIT FUNCTION, TO DIVIDE OUR X AND Y VARIABLE INTO TRAINING AND TESTING SET \n",
    "(THE OUTPUT OF FUNCTION IS 2 VARIABLES, ONE CORRESSPONDING TO ALL THE INDEXES IN THE X AND Y VARIABLE THAT ARE PART OF THE TRAINING SET, THE OTHER CORRESSPONDING TO ALL THE INDEXES IN THE X AND Y VARIABLE THAT ARE PART OF THE TESTING SET  )\n",
    "THEN I APPLY FEATURE SCALING ON THE TRAINING PART OF THE X VARIABLE SO AS TO CHANGE THE RANGE OF THE VALUES STORED IN IT, WHICH IN TURN HELPS THE MODEL TRAIN BETTER\n",
    "AFTER THIS I USE THE SAME SCALING TECHNIQUE ON THE TEST PART OF THE X VARIABLE SO THAT MODEL CAN USE IT PREVIOUS KNOWLEDGE PROPERLY WHILE PREDICTING OUTPUT FOR THESE VALUES.\n",
    "NOW I TRAIN THE CLASSIFIER BASED ON THE TRAINING PORTION OF THE X AND Y VARIABLE.\n",
    "AFTER WHICH I USE THE TRAIN MODEL TO PREDICT THE OUTPUT FOR THE TEST PORTION OF THE X VARIABLES\n",
    "I USE THIS PREDICTION TO DEVELOP A CONFUSION MATRIX ALONG WITH THE ACTUAL OUTPUT OF THE TEST(I.E, THE TEST PORTION OF THE Y VARIABLES)\n",
    "THEN I USE THE CONFUSION MATRIX TO RETRIVE THE TRUE POSITVE, FALSE POSITIVE, TRUE NEGATIVE AND FALSE NEGATIVE FOR OUR PREDICTION.\n",
    "I USE THIS TO CALCULATE THE FALSE_POSITIVE_RATE, FALSE_NEGATIVE_RATE, OVERALL_ERROR_RATE OF OUR ALGORITHM FOR A PARTICULAR FOLD AND THEN PRINT THOSE RESULTS IN A TABULAR FASION.\n",
    "I ALSO STORE THESE VALUES IN OUR LISTS THAT WE HAD INITALIZED EARLIER TO KEEP TRACK OF ALL THE ALGORITHMS OVERALL PERFORMANCE.\n",
    "AFTER EXITING THE KFOLD LOOP I DIVIDE THE LIST VALUES BY 5 SO AS TO GET THE AVERAGE PERFORMANCE OF EACH ALGORITHM ACROSS 5 FOLDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Best Classification Algorithm: Random_Forest | Average Overall Error Rate: 0.04433885663031676 | Average False Positive Rate: 0.027362256473534914 | Average False Negative Rate: 0.07019545280323523\n"
     ]
    }
   ],
   "source": [
    "print('\\n')        \n",
    "min_value = min(average_overall_error_rate)\n",
    "min_index = average_overall_error_rate.index(min_value)\n",
    "print('Best Classification Algorithm: '+Classification_Algorithms[min_index]+' | Average Overall Error Rate: '+str(average_overall_error_rate[min_index])+' | Average False Positive Rate: '+str(average_false_positive_rate[min_index])+' | Average False Negative Rate: '+str(average_false_negative_rate[min_index]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I DECIDED TO CHOOSE THE ALGORITHM THAT HAD THE LOWEST AVERAGE OVERALL ERROR.\n",
    "I USED THE MIN FUNCTION TO FIND THE LOWEST VALUE IN THE 'average_overall_error_rate' LIST\n",
    "THEN I USED THIS VALUE TO FIND ITS INDEX POSITION IN THE LIST.\n",
    "USING THE INDEX POSITION I PRINT THE BEST PERFORMING ALGORITHM AND ITS OVERALL RESULST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
